{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:31.600958Z",
     "start_time": "2024-07-30T21:09:31.594016Z"
    }
   },
   "cell_type": "code",
   "source": "# **Import Libraries** \n",
   "id": "d2fce8391ae60abc",
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:32.016237Z",
     "start_time": "2024-07-30T21:09:31.994718Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import unicodeit\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display,Markdown\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score,root_mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dense, Concatenate, Flatten,Reshape,SimpleRNN\n",
    "from keras_self_attention import SeqWeightedAttention\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:32.159549Z",
     "start_time": "2024-07-30T21:09:32.054404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define random seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ],
   "id": "2f075384ae56f608",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1. Load the \"Individual household electric power consumption\" dataset** \n",
    "***"
   ],
   "id": "65ac0a8f630e9313"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ddee0912ccf02b80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:32.690432Z",
     "start_time": "2024-07-30T21:09:32.160737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# first define datatypes for the columns\n",
    "dtypes = {'Date': str,\n",
    "          'Time': str,\n",
    "          'Global_active_power': float,\n",
    "          'Global_reactive_power': float,\n",
    "          'Voltage': float,\n",
    "          'Global_intensity': float,\n",
    "          'Sub_metering_1': float,\n",
    "          'Sub_metering_2': float,\n",
    "          'Sub_metering_3': float}\n",
    "\n",
    "'''\n",
    "fetch the dataset (acknowledging that the seperator is ';' and missing values are marked as '?'\n",
    "'''\n",
    "\n",
    "file_path = 'household_power_consumption.txt'\n",
    "power_consumption_df = pd.read_csv(file_path, sep=';',dtype=dtypes,na_values='?')\n",
    "power_consumption_df.head()"
   ],
   "id": "2126476087153fe8",
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 17\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03mfetch the dataset (acknowledging that the seperator is ';' and missing values are marked as '?'\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     16\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhousehold_power_consumption.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 17\u001B[0m power_consumption_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m;\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43mna_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m power_consumption_df\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[1;32m~\\Desktop\\personal\\Data_Science\\afeka\\שנה א סימסטר ב\\LAB2\\EX2\\venv_2\\Hw2_venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\personal\\Data_Science\\afeka\\שנה א סימסטר ב\\LAB2\\EX2\\venv_2\\Hw2_venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\personal\\Data_Science\\afeka\\שנה א סימסטר ב\\LAB2\\EX2\\venv_2\\Hw2_venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\Desktop\\personal\\Data_Science\\afeka\\שנה א סימסטר ב\\LAB2\\EX2\\venv_2\\Hw2_venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2061\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that we have 9 features:\n",
    "* Date\n",
    "* Time\n",
    "* Global_active_power\n",
    "* Global_reactive_power\n",
    "* Voltage\n",
    "* Global_intensity\n",
    "* Sub_metering_1\n",
    "* Sub_metering_2\n",
    "* Sub_metering_3\n",
    "\n",
    "However, our target (active energy consumed every minute) is  not specified in the dataset. we will define \"Global_active_power\" as our target. Also, we would like to combine date and time to a one feature called: \"datetime\"."
   ],
   "id": "c8029e6a19526e8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:32.691085Z",
     "start_time": "2024-07-30T21:09:32.691085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# combine date and time into datetime\n",
    "power_consumption_df['Datetime'] = pd.to_datetime(power_consumption_df['Date'] + ' ' + power_consumption_df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Set the datetime column as index\n",
    "power_consumption_df.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Drop the original Date and TIme columns\n",
    "power_consumption_df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "power_consumption_df.head()"
   ],
   "id": "2feed627e34f4aa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. **EDA**\n",
    "***"
   ],
   "id": "f0fd42a2104970b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a correlation plot to find the correlation between columns\n",
    "plt.figure(figsize=(10, 10)) \n",
    "sns.heatmap(power_consumption_df.corr(method='pearson'),  \n",
    "            annot=True,  \n",
    "            cmap='RdYlGn',  \n",
    "            fmt='.2f')  \n",
    "plt.show()"
   ],
   "id": "ae56edb2a1c28b0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define colors for plotting\n",
    "colors = sns.color_palette(\"Set2\", len(power_consumption_df .columns))\n",
    "\n",
    "# Define number of plots per row\n",
    "plots_per_row = 5\n",
    "\n",
    "# Calculate number of rows needed for the subplot grid\n",
    "num_rows = len(power_consumption_df .columns) // plots_per_row + (len(power_consumption_df .columns) % plots_per_row > 0)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=plots_per_row, figsize=(20, 15))\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, (col, color) in enumerate(zip(power_consumption_df .columns, colors)):\n",
    "    power_consumption_df [col].hist(bins=50, ax=axes[i], color=color)\n",
    "    axes[i].set_title(f\"{col} Distribution\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Remove unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "815ecf85428726cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the distributions we can draw the following conclusions:\n",
    "* The Global Active power seems to be consisted of two different normal distributions, one centered near 0, and the other centered at approximately 1.7. That could imply some different seasonal patterns within the year, one with lower power consumption, and the other - with a higher one.\n",
    "\n",
    "* The Global reactive power seems to be less sparse, and will most likely be near 0. It implies that the electricity network is efficient and doesn't suffer from many losses :) .\n",
    "\n",
    "* The voltage is clearly normally distributed with a mean around 240V (which makes sense since it's a French household). The voltage consumption is sparse, and that could be explained by different voltage consumptions from different appliances.\n",
    "\n",
    "* Just like Global Active Power, the Global Intensity also seems to be consisted of two normal distributions which can imply different seasonal patterns - one centered at approximately 0 and one centered at approximately 5 .\n",
    "\n",
    "* Sub_metering_1, Sub_metering_2, Sub_metering_3: High frequency of zero values indicates many periods with no sub-metered energy usage, with some occasional spikes."
   ],
   "id": "a9ee5d457d8f12cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation: ###\n",
    "***\n",
    "\n",
    "* Global_active_power: Skewed to the right, indicating that most values are low, but there are occasional high values.\n",
    "* Voltage: More normally distributed, showing stable voltage supply.\n",
    "* Sub_metering_1, Sub_metering_2, Sub_metering_3: High frequency of zero values indicates many periods with no sub-metered energy usage, with some occasional spikes."
   ],
   "id": "776ac5f8e51ec253"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_time_trends(data,columns_to_omit=None,resample=None):\n",
    "    num_plots = len(data.columns) - len(columns_to_omit) if columns_to_omit is not None else len(data.columns)\n",
    "    num_rows = num_cols = math.ceil(math.sqrt(num_plots))\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15,10))\n",
    "    if columns_to_omit is  None:\n",
    "        columns_to_omit = []\n",
    "\n",
    "    i = j = 0\n",
    "\n",
    "    for column in data.columns:\n",
    "        if column not in columns_to_omit:\n",
    "            if resample is None:\n",
    "                axes[i, j].plot(data[column], label=column)\n",
    "\n",
    "            else:\n",
    "                axes[i, j].plot(data[column].resample(resample).mean(), label=column)\n",
    "\n",
    "            axes[i, j].set_title(f'Time series for {column}')\n",
    "            axes[i, j].set_xlabel('Datetime')\n",
    "            axes[i, j].set_ylabel(f\"value\")\n",
    "            axes[i,j].legend()\n",
    "            j += 1\n",
    "\n",
    "            if j == num_cols:\n",
    "                j = 0\n",
    "                i += 1\n",
    "\n",
    "    if math.sqrt(num_plots) < num_cols:\n",
    "        remainder = (num_cols ** 2) % num_plots\n",
    "        for i in range(1, remainder + 1, 1):\n",
    "            axes[-1,-1 * i].axis('off') # hide the last plots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "visualize_time_trends(power_consumption_df)"
   ],
   "id": "baea14d9f61e1dae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "According to the time series trends we can say the following:\n",
    "1. **Global Active Power**\n",
    "\n",
    "    * **Seasonality:** It seems like there are strong seasonal patterns with regular peaks and troughs (best appeared in the monthly resampling). This phenomenon could be explained by high power consumption during certain times of the year (for example, hot summer days)\n",
    "         \n",
    "   * **Cyclical Patterns:** There may be some longer-term trends, but they are overshadowed and difficult to isolate from the clear seasonal patterns without further analysis.\n",
    "\n",
    "2. **Global reactive power**\n",
    "    * **Seasonality:** Similar to Global Active Power, we can see a strong seasonal pattern (which makes sense, since the reactive power are the losses from the electrical appliances of the consumer, so they should have the same seasonality as the active power)\n",
    "      \n",
    "    * **Cyclical Patterns:** The same as the active power - longer term trends are less apparent, since there is a strong seasonal pattern.\n",
    "\n",
    "3. **Voltage** \n",
    "    * **Seasonality:** The voltage shows less clear seasonal patterns (it doesn't seem to have a constant period) compared to power consumption, though there are still some periodic fluctuations (it seems clearer in the weekly or monthly resampling)\n",
    "      \n",
    "    * **Cyclical Patterns:** If there are any cyclical patterns in the voltage, they are not very prominent.\n",
    "\n",
    "4. **Global Intensity**\n",
    "    * **Seasonality:** There is a clear seasonal pattern, with regular fluctuations which again could be explained by periods of higher electricity demand.\n",
    "      \n",
    "    * **Cyclical Patterns:** Similar to the Global Active Power, the cyclical trends are less evident due to the dominant seasonal patterns.\n",
    "\n",
    "5. **Sub Metering 1,2, and 3:** \n",
    "    * **Seasonality:** Each sub metering shows distinct seasonal patterns, likely corresponding to specific appliances that have different regular usage cycles.\n",
    "      \n",
    "    * **Cyclical Patterns:** Less apparent, as the data is dominated by strong seasonal patterns.\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "This dataset exhibits strong seasonal patterns across most of the features, as well as the target variable (Global_active_power). The seasonal patterns are consistent and predictable, and most likely correspond to higher and lower power demand during different periods of the year.\n",
    "\n",
    "Cyclical patterns in this dataset are harder to distinguish from the seasonal ones, due to the dominance of the seasonal patterns."
   ],
   "id": "4382540290f4e2f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Identify and handle missing values and outliers**\n",
    "***"
   ],
   "id": "72ffa55b42cf841a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(power_consumption_df.isna().sum())\n",
    "display(Markdown(f'##### {power_consumption_df.isna().sum().iloc[0]/ power_consumption_df.shape[0] * 100}% of the data is missing'))"
   ],
   "id": "a9b31ffbad9b4cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ee94074d6977582b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "power_consumption_df_no_missing = power_consumption_df.copy()\n",
    "\n",
    "for _column in power_consumption_df_no_missing.columns:\n",
    "    # forward fill the column\n",
    "    power_consumption_df_no_missing[_column] = power_consumption_df_no_missing[_column].ffill() \n",
    "\n",
    "# check for the existence of missing values again\n",
    "power_consumption_df_no_missing.isna().sum()"
   ],
   "id": "9dff7f0fa63d7540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have filled the missing values and re-calculated our target. let's deal with the outliers. let's make a seasonal decomposition (for example, let's do that to the global_active_power):",
   "id": "4262009480245e72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "power_consumption_df_no_missing = power_consumption_df.copy()\n",
    "\n",
    "for _column in power_consumption_df_no_missing.columns:\n",
    "    # forward fill the column\n",
    "    power_consumption_df_no_missing[_column] = power_consumption_df_no_missing[_column].ffill() \n",
    "\n",
    "# check for the existence of missing values again\n",
    "power_consumption_df_no_missing.isna().sum()"
   ],
   "id": "e496cbbaa32bfb40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have filled the missing values and re-calculated our target. let's deal with the outliers. let's make a seasonal decomposition (for example, let's do that to the global_active_power):",
   "id": "14739e630823ad40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(power_consumption_df_no_missing['Global_active_power'],model='additive',period=365)\n",
    "fig1 = decomposition.plot()\n",
    "fig1.set_size_inches((15,10))\n",
    "plt.show()"
   ],
   "id": "3558da3a61d4b42c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This decomposition consists of the following:\n",
    "* **Observed:** The time series as it is (the actual data)\n",
    "* **Trend:** reveals the long-term increase/decrease in the global_active_power consumption.\n",
    "* **Seasonal:** The seasonal plot shows recurring patterns, for example: higher consumption during certain times of the month.\n",
    "* **Residual:** The residual plot displays the noise or irregularities in the data (so we can use that in order to identify the outliers). We wil define an outlier as:\n",
    "$outlier = resid.mean() \\pm 3*resid.std()$\n",
    "* We will check outliers for our target variable to see if it makes sense (for example - higher power consumption during summer). "
   ],
   "id": "1824d499dbe8f466"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_for_outliers(time_series,period,number_of_stds):\n",
    "    seasonal_decomposition = sm.tsa.seasonal_decompose(time_series,model='additive',period=period)\n",
    "    residuals = seasonal_decomposition.resid\n",
    "    outliers_high = residuals[(residuals > residuals.mean() + number_of_stds * residuals.std())]\n",
    "    outliers_low = residuals[(residuals < residuals.mean() - number_of_stds * residuals.std())]\n",
    "    return outliers_low.index, outliers_high.index\n",
    "(low_outliers_dates, high_outliers_dates) = check_for_outliers(time_series=power_consumption_df_no_missing['Global_active_power'],period=365,number_of_stds=3)\n",
    "display(Markdown(f'##### low_power consumption dates are: {low_outliers_dates}'))\n",
    "display(Markdown(f'##### high_power consumption dates are: {high_outliers_dates}'))"
   ],
   "id": "8c04be07959c3646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "let's see what percentage of the data are these outliers before we make a decision:",
   "id": "38cfa6194d7fc352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(Markdown(f'##### {(len(low_outliers_dates) + len(high_outliers_dates)) / power_consumption_df_no_missing.shape[0] * 100}% of the data is considered outliers'))",
   "id": "9103c42d997cae81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "since only 2% of the data is considered outliers, we will decide to drop them:\n",
   "id": "2554f57d36092da9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "power_consumption_df_no_missing_no_outliers = power_consumption_df_no_missing.copy()\n",
    "outliers_to_drop = low_outliers_dates.union(high_outliers_dates)\n",
    "power_consumption_df_no_missing_no_outliers.drop(outliers_to_drop,inplace=True)\n",
    "display(power_consumption_df_no_missing_no_outliers)"
   ],
   "id": "5411dfc39a8aaad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy= power_consumption_df_no_missing_no_outliers.copy\n",
    "\n"
   ],
   "id": "217c2edce6ecc8ac",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Implement a Linear Regression Model #\n",
    "***\n"
   ],
   "id": "fc3109b40349618f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the data into training and testing sets\n",
   "id": "f0ef6149bd33663a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:32.726031Z",
     "start_time": "2024-07-30T21:09:32.709900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create lagged features\n",
    "def create_lagged_features(df, columns, lags):\n",
    "    for column in columns:\n",
    "        for lag in range(1, lags + 1):\n",
    "            df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
    "    return df.dropna()\n",
    "\n",
    "# Function to split the data into train and test sets (80:20 ratio)\n",
    "def split_data(df, target_column):\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train, test = df[:train_size], df[train_size:]\n",
    "    X_train = train.drop(target_column, axis=1)\n",
    "    y_train = train[target_column]\n",
    "    X_test = test.drop(target_column, axis=1)\n",
    "    y_test = test[target_column]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to resample data\n",
    "def resample_data(df, freq):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be of datetime type\")\n",
    "    return df.resample(freq).mean()\n",
    "\n",
    "# Function to prepare features and train linear regression model\n",
    "\n",
    "# Function to prepare features and train linear regression model # 'minutely': 'min','hourly': 'h', 'daily': 'D','monthly': 'ME','weekly': 'W'\n",
    "def train_and_evaluate_linear_regression(df, target_column, lags, resample_freq='D'):\n",
    "    # Resample data\n",
    "    resampled_df = resample_data(df, resample_freq)\n",
    "    \n",
    "    # Create lagged features\n",
    "    columns_selected = ['Global_active_power']\n",
    "    processed_df = create_lagged_features(resampled_df, columns_selected, lags)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    columns_to_drop = ['Global_reactive_power', 'Voltage', 'Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in processed_df.columns]\n",
    "    processed_df = processed_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(processed_df, target_column)\n",
    "    \n",
    "    # Train linear regression model\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    y_pred = linear_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mse, mae, rmse, r2, y_test, y_pred\n",
    "\n",
    "\n",
    "# Function to plot actual vs predicted values for different time periods\n",
    "def plot_actual_vs_predicted(y_test, y_pred, model_name):\n",
    "    time_periods = [50, 100, 200, 300]\n",
    "    fig2, axes2 = plt.subplots(2, 2, figsize=(20, 14))\n",
    "    axes2 = axes2.flatten()\n",
    "\n",
    "    for i, period in enumerate(time_periods):\n",
    "        axes2[i].plot(y_test.index[-period:], y_test[-period:], label='Actual', color='blue')\n",
    "        axes2[i].plot(y_test.index[-period:], y_pred[-period:], label='Predicted', color='red', linestyle='dashed')\n",
    "        axes2[i].set_title(f'Actual vs Predicted Global Active Power (Last {period} Data Points) - {model_name}')\n",
    "        axes2[i].set_xlabel('DateTime')\n",
    "        axes2[i].set_ylabel('Global Active Power (kilowatts)')\n",
    "        axes2[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "5efef0ae6495f8d3",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(results_df)",
   "id": "dcade21c57dcb503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "we can see that the RNN model performed poorly comparing to the LinearRegression model.There could be some reasons for that:\n",
    "* Lack of complex patterns - If the time series lacks intricate patterns or long-term dependencies, the complexity of an RNN may not offer any advantage over LinearRegression and could even hinder performance.\n",
    "* Overfitting - The data is noisy (as we could see from the time series), Therefore a complex model (such as RNN) may lead to overfitting.\n",
    "* Size of the dataset - LinearRegression will perform better on a smaller dataset, while RNN requires more data to learn effectively. Our dataset is large, but we resampled and perhaps under sampled it for RNN.\n",
    "* Training Complexity - RNN is more challenging to train due to issues like vanishing/exploding gradients.\n",
    "* Sensitivity to Hyperparameters - RNN is highly sensitive to hyperparameters (e.g. learning rate, sequence length, number of layers, hidden units). Poor tuning may lead the RNN to a bad performance, while LinearRegression model often require less hyperparameter tuning.\n",
    "* Optimization Challenges - RNN may face optimization challenges that are less prominent in linear regression models.\n",
    "* Feature Engineering - Linear Regression can benefit significantly from good feature engineering, which can sometimes be more intuitive. Creating lag variables (as we did) might make the linear model very powerful."
   ],
   "id": "bfae720bafa4aecf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LSTM #\n",
    "***"
   ],
   "id": "40b03f63f46041da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T21:09:38.527885Z",
     "start_time": "2024-07-30T21:09:38.527885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to reshape data for LSTM input\n",
    "def reshape_data_for_lstm(X_train, X_test):\n",
    "    X_train_reshaped = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_reshaped = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    return X_train_reshaped, X_test_reshaped\n",
    "\n",
    "# Function to prepare features and train LSTM model\n",
    "def train_and_evaluate_lstm(df, target_column, lags, resample_freq='D', epochs=10, batch_size=32):\n",
    "    # Ensure DataFrame index is datetime\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be of datetime type\")\n",
    "\n",
    "    # Resample data\n",
    "    resampled_df = resample_data(df, resample_freq)\n",
    "    \n",
    "    # Create lagged features\n",
    "    columns_selected = ['Global_active_power','Global_reactive_power', 'Voltage', 'Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']\n",
    "    processed_df = create_lagged_features(resampled_df, columns_selected, lags)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    columns_to_drop = ['Global_reactive_power', 'Voltage', 'Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in processed_df.columns]\n",
    "    processed_df = processed_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(processed_df, target_column)\n",
    "    \n",
    "    # Reshape data for LSTM input\n",
    "    X_train_reshaped, X_test_reshaped = reshape_data_for_lstm(X_train, X_test)\n",
    "    \n",
    "    # Build LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "    lstm_model.add(LSTM(50))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train LSTM model\n",
    "    lstm_model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = lstm_model.predict(X_test_reshaped)\n",
    "    \n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mse, mae, rmse, r2, y_test, y_pred\n",
    "\n"
   ],
   "id": "1657991be98bc532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "target_column = 'Global_active_power'\n",
    "lags = 3\n",
    "resample_freq = ('min')  # Change this to 'T', 'H', 'D', 'W', 'min' as needed\n",
    "\n",
    "# Train and evaluate LSTM model\n",
    "mse, mae, rmse, r2, y_test, y_pred = train_and_evaluate_lstm(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq)\n",
    "# Print results\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R²: {r2}')\n",
    "\n",
    "plot_actual_vs_predicted(y_test, y_pred, 'LSTM')\n",
    "results_df = pd.concat([results_df,pd.DataFrame({\"Model_Name\": \"LSTM\", \"MSE\": mse, \"MAE\": mae, \"RMSE\":rmse, unicodeit.replace('R^2'): r2},index=[0])],axis=0, ignore_index=True)"
   ],
   "id": "35fa496b8672f16c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Compare Results**\n",
    "***"
   ],
   "id": "2c2ac275dd47c827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(results_df)",
   "id": "7c93602770511ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The RNN and the LSTM are very similar, and the Linear regression still wins.",
   "id": "828a812a3030a18e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LSTM with Attention #\n",
    "***"
   ],
   "id": "b99daf1f01117e54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Function to build and train an LSTM model with Attention\n",
    "def train_and_evaluate_attention_lstm(df, target_column, lags, resample_freq='D', epochs=10, batch_size=32):\n",
    "    # Resample data\n",
    "    resampled_df = resample_data(df, resample_freq)\n",
    "    \n",
    "    # Create lagged features\n",
    "    columns_selected = ['Global_active_power']\n",
    "    processed_df = create_lagged_features(resampled_df, columns_selected, lags)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    columns_to_drop = ['Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in processed_df.columns]\n",
    "    processed_df = processed_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(processed_df, target_column)\n",
    "    \n",
    "    # Reshape data for LSTM input\n",
    "    X_train_reshaped, X_test_reshaped = reshape_data_for_rnn(X_train, X_test)\n",
    "    \n",
    "    # Build LSTM model with Attention\n",
    "    input_layer = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
    "    lstm_out, state_h, state_c = LSTM(50, return_sequences=True, return_state=True)(input_layer)\n",
    "    \n",
    "    attention = SeqWeightedAttention()(lstm_out)\n",
    "    # Reshape lstm_out to match the attention output shape\n",
    "    attention_reshaped = Reshape((1, 50))(attention)\n",
    "    context_vector = Concatenate(axis=-1)([attention_reshaped, lstm_out])\n",
    "    context_vector = Flatten()(context_vector)\n",
    "    output = Dense(1)(context_vector)\n",
    "    attention_lstm_model = Model(inputs=input_layer, outputs=output)\n",
    "    attention_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train LSTM model with Attention\n",
    "    attention_lstm_model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = attention_lstm_model.predict(X_test_reshaped)\n",
    "    \n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mse, mae, rmse, r2, y_test, y_pred\n",
    "\n"
   ],
   "id": "2a5b5a672c427b5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "target_column = 'Global_active_power'\n",
    "lags = 3\n",
    "resample_freq = ('h')  # Change this to 'T', 'H', 'D', 'W', 'min' as needed\n",
    "\n",
    "\n",
    "mse_att_lstm, mae, rmse, r2, y_test, y_pred = train_and_evaluate_attention_lstm(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq)\n",
    "\n",
    "# Print results\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R²: {r2}')\n",
    "plot_actual_vs_predicted(y_test, y_pred, 'Attention LSTM')\n",
    "results_df = pd.concat([results_df,pd.DataFrame({\"Model_Name\": \"LSTM with attention\", \"MSE\": mse, \"MAE\": mae, \"RMSE\":rmse, unicodeit.replace('R^2'): r2},index=[0])],axis=0, ignore_index=True)"
   ],
   "id": "b390e553ebb105d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Compare Results**\n",
    "***"
   ],
   "id": "352faa3fd26b0fa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(results_df)",
   "id": "af990edbe1e6ec31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It Doesn't seem like the attention layer had a meaningful effect to the LSTM model. The linear regression model still wins.",
   "id": "6924315fb289edc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation  ##\n",
    "***"
   ],
   "id": "5847417cffd12058"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to augment data\n",
    "def augment_data(df, max_change=0.1):\n",
    "    augmented_df = df.copy()\n",
    "    num_rows_to_modify = int(len(df) * max_change)\n",
    "    rows_to_modify = random.sample(range(len(df)), num_rows_to_modify)\n",
    "    \n",
    "    for row in rows_to_modify:\n",
    "        augmented_df.iloc[row] *= random.uniform(0.9, 1.1)  # Apply random change within ±10%\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "def experiment_with_augmentation(df, target_column, lags, resample_freq, max_change=0.1, epochs=10, batch_size=32):\n",
    "    augmented_df = augment_data(df, max_change)\n",
    "    \n",
    "    # Train and evaluate Attention LSTM model with augmented data\n",
    "    attention_lstm_mse, attention_lstm_mae, attention_lstm_rmse, attention_lstm_r2, y_test_attention_lstm_aug, y_pred_attention_lstm_aug = train_and_evaluate_attention_lstm(augmented_df, target_column, lags, resample_freq, epochs, batch_size)\n",
    "    \n",
    "    # Train and evaluate LSTM model with augmented data\n",
    "    lstm_mse, lstm_mae, lstm_rmse, lstm_r2, y_test_lstm_aug, y_pred_lstm_aug = train_and_evaluate_lstm(augmented_df, target_column, lags, resample_freq, epochs, batch_size)\n",
    "    \n",
    "    # Train and evaluate RNN model with augmented data\n",
    "    rnn_mse, rnn_mae, rnn_rmse, rnn_r2, y_test_rnn_aug, y_pred_rnn_aug = train_and_evaluate_rnn(augmented_df, target_column, lags, resample_freq, epochs, batch_size)\n",
    "    \n",
    "    return {\n",
    "        \"aug_attention_lstm\": (attention_lstm_mse, attention_lstm_mae, attention_lstm_rmse, attention_lstm_r2, y_test_attention_lstm_aug, y_pred_attention_lstm_aug),\n",
    "        \"aug_lstm\": (lstm_mse, lstm_mae, lstm_rmse, lstm_r2, y_test_lstm_aug, y_pred_lstm_aug),\n",
    "        \"aug_rnn\": (rnn_mse, rnn_mae, rnn_rmse, rnn_r2, y_test_rnn_aug, y_pred_rnn_aug)\n",
    "    }"
   ],
   "id": "f8d8b6ef200873a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Reduction  ##\n",
    "***"
   ],
   "id": "6e9e122d8e56d06b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reduce_data(df, reduction_percentage=0.1):\n",
    "    reduced_df = df.sample(frac=(1 - reduction_percentage), random_state=42).sort_index()\n",
    "    return reduced_df\n",
    "\n",
    "def experiment_with_reduction(df, target_column, lags, resample_freq, reduction_percentage=0.1, epochs=10, batch_size=32):\n",
    "    reduced_df = reduce_data(df, reduction_percentage)\n",
    "    \n",
    "    # Train and evaluate Attention LSTM model with reduced data\n",
    "    attention_lstm_mse, attention_lstm_mae, attention_lstm_rmse, attention_lstm_r2, y_test_attention_lstm_red, y_pred_attention_lstm_red = train_and_evaluate_attention_lstm(reduced_df, target_column, lags, resample_freq, epochs, batch_size)\n",
    "    \n",
    "    # Train and evaluate LSTM model with reduced data\n",
    "    lstm_mse, lstm_mae, lstm_rmse, lstm_r2, y_test_lstm_red, y_pred_lstm_red = train_and_evaluate_lstm(reduced_df, target_column, lags, resample_freq, epochs, batch_size)\n",
    "    \n",
    "    # Train and evaluate RNN model with reduced data\n",
    "    rnn_mse, rnn_mae, rnn_rmse, rnn_r2, y_test_rnn_red, y_pred_rnn_red = train_and_evaluate_rnn(reduced_df, target_column, lags, resample_freq, epochs, batch_size)\n",
    "    \n",
    "    return {\n",
    "        \"red_attention_lstm\": (attention_lstm_mse, attention_lstm_mae, attention_lstm_rmse, attention_lstm_r2, y_test_attention_lstm_red, y_pred_attention_lstm_red),\n",
    "        \"red_lstm\": (lstm_mse, lstm_mae, lstm_rmse, lstm_r2, y_test_lstm_red, y_pred_lstm_red),\n",
    "        \"red_rnn\": (rnn_mse, rnn_mae, rnn_rmse, rnn_r2, y_test_rnn_red, y_pred_rnn_red)\n",
    "    }\n"
   ],
   "id": "b396646929527ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run Models With Data Augmentation Experiment ##\n",
    "***"
   ],
   "id": "613091fcbc77a413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "target_column = 'Global_active_power'\n",
    "lags = 3\n",
    "resample_freq = 'h'\n",
    "max_change = 0.1\n",
    "epochs = 10  # You need to define epochs and batch_size for the function calls\n",
    "batch_size = 32\n",
    "\n",
    "# Original model training and evaluation\n",
    "attention_lstm_mse, attention_lstm_mae, attention_lstm_rmse, attention_lstm_r2, y_test_attention_lstm, y_pred_attention_lstm = train_and_evaluate_attention_lstm(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq, epochs, batch_size)\n",
    "\n",
    "lstm_mse, lstm_mae, lstm_rmse, lstm_r2, y_test_lstm, y_pred_lstm = train_and_evaluate_lstm(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq, epochs, batch_size)\n",
    "\n",
    "rnn_mse, rnn_mae, rnn_rmse, rnn_r2, y_test_rnn, y_pred_rnn = train_and_evaluate_rnn(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq, epochs, batch_size)\n",
    "\n",
    "# Data augmentation experiment\n",
    "aug_results = experiment_with_augmentation(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq, max_change, epochs, batch_size)\n",
    "\n",
    "# Update results DataFrame\n",
    "results_df = pd.DataFrame(columns=[\"Model_Name\", \"MSE\", \"MAE\", \"RMSE\", 'R^2'])\n",
    "\n",
    "# Original models\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    \"Model_Name\": [\"Attention LSTM\", \"LSTM\", \"RNN\"], \n",
    "    \"MSE\": [attention_lstm_mse, lstm_mse, rnn_mse], \n",
    "    \"MAE\": [attention_lstm_mae, lstm_mae, rnn_mae], \n",
    "    \"RMSE\": [attention_lstm_rmse, lstm_rmse, rnn_rmse], \n",
    "    'R^2': [attention_lstm_r2, lstm_r2, rnn_r2]\n",
    "})], axis=0, ignore_index=True)\n",
    "\n",
    "# Augmented models\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    \"Model_Name\": [\"Attention LSTM Augmented\", \"LSTM Augmented\", \"RNN Augmented\"], \n",
    "    \"MSE\": [aug_results[\"aug_attention_lstm\"][0], aug_results[\"aug_lstm\"][0], aug_results[\"aug_rnn\"][0]], \n",
    "    \"MAE\": [aug_results[\"aug_attention_lstm\"][1], aug_results[\"aug_lstm\"][1], aug_results[\"aug_rnn\"][1]], \n",
    "    \"RMSE\": [aug_results[\"aug_attention_lstm\"][2], aug_results[\"aug_lstm\"][2], aug_results[\"aug_rnn\"][2]], \n",
    "    'R^2': [aug_results[\"aug_attention_lstm\"][3], aug_results[\"aug_lstm\"][3], aug_results[\"aug_rnn\"][3]]\n",
    "})], axis=0, ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "display(results_df)\n"
   ],
   "id": "1bfc349eb711cabb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run Models With Data reduction Experiment ##\n",
    "***"
   ],
   "id": "2060da2d3f454b45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Experiment with reduced data\n",
    "red_results = experiment_with_reduction(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq)# Train and evaluate Attention LSTM model with reduced data\n",
    "\n",
    "\n",
    "# Update results DataFrame\n",
    "results_df = pd.DataFrame(columns=[\"Model_Name\", \"MSE\", \"MAE\", \"RMSE\", 'R^2'])\n",
    "\n",
    "# Original models\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    \"Model_Name\": [\"Attention LSTM\", \"LSTM\", \"RNN\"], \n",
    "    \"MSE\": [attention_lstm_mse, lstm_mse, rnn_mse], \n",
    "    \"MAE\": [attention_lstm_mae, lstm_mae, rnn_mae], \n",
    "    \"RMSE\": [attention_lstm_rmse, lstm_rmse, rnn_rmse], \n",
    "    'R^2': [attention_lstm_r2, lstm_r2, rnn_r2]\n",
    "})], axis=0, ignore_index=True)\n",
    "\n",
    "# Augmented models\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    \"Model_Name\": [\"Attention LSTM Augmented\", \"LSTM Augmented\", \"RNN Augmented\"], \n",
    "    \"MSE\": [aug_results[\"aug_attention_lstm\"][0], aug_results[\"aug_lstm\"][0], aug_results[\"aug_rnn\"][0]], \n",
    "    \"MAE\": [aug_results[\"aug_attention_lstm\"][1], aug_results[\"aug_lstm\"][1], aug_results[\"aug_rnn\"][1]], \n",
    "    \"RMSE\": [aug_results[\"aug_attention_lstm\"][2], aug_results[\"aug_lstm\"][2], aug_results[\"aug_rnn\"][2]], \n",
    "    'R^2': [aug_results[\"aug_attention_lstm\"][3], aug_results[\"aug_lstm\"][3], aug_results[\"aug_rnn\"][3]]\n",
    "})], axis=0, ignore_index=True)\n",
    "\n",
    "# Reduced models\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    \"Model_Name\": [\"Attention LSTM Reduced\", \"LSTM Reduced\", \"RNN Reduced\"], \n",
    "    \"MSE\": [red_results[\"red_attention_lstm\"][0], red_results[\"red_lstm\"][0], red_results[\"red_rnn\"][0]], \n",
    "    \"MAE\": [red_results[\"red_attention_lstm\"][1], red_results[\"red_lstm\"][1], red_results[\"red_rnn\"][1]], \n",
    "    \"RMSE\": [red_results[\"red_attention_lstm\"][2], red_results[\"red_lstm\"][2], red_results[\"red_rnn\"][2]], \n",
    "    'R^2': [red_results[\"red_attention_lstm\"][3], red_results[\"red_lstm\"][3], red_results[\"red_rnn\"][3]]\n",
    "})], axis=0, ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "display(results_df)\n"
   ],
   "id": "4a1fb0ba12dbabc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "907eba2c32ce0348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "target_column = 'Global_active_power'\n",
    "lags = 3\n",
    "resample_freq = '2h'\n",
    "\n",
    "# Original model training and evaluation\n",
    "attention_lstm_mse, attention_lstm_mae, attention_lstm_rmse, attention_lstm_r2, y_test_attention_lstm, y_pred_attention_lstm = train_and_evaluate_attention_lstm(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq)\n",
    "\n",
    "lstm_mse, lstm_mae, lstm_rmse, lstm_r2, y_test_lstm, y_pred_lstm = train_and_evaluate_lstm(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq)\n",
    "\n",
    "rnn_mse, rnn_mae, rnn_rmse, rnn_r2, y_test_rnn, y_pred_rnn = train_and_evaluate_rnn(power_consumption_df_no_missing_no_outliers, target_column, lags, resample_freq )"
   ],
   "id": "6daec218331bc4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Original models\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    \"Model_Name\": [\"Attention LSTM\", \"LSTM\", \"RNN\"], \n",
    "    \"MSE\": [attention_lstm_mse, lstm_mse, rnn_mse], \n",
    "    \"MAE\": [attention_lstm_mae, lstm_mae, rnn_mae], \n",
    "    \"RMSE\": [attention_lstm_rmse, lstm_rmse, rnn_rmse], \n",
    "    'R^2': [attention_lstm_r2, lstm_r2, rnn_r2]\n",
    "})], axis=0, ignore_index=True)\n",
    "\n",
    "display(results_df)"
   ],
   "id": "9a5d08b3a4c17ea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8c1f34cc219a1c56",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
